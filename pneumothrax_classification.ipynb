{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pneumothrax_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qyltv_PsOGohvu67ZJG2csrcBeP1ZDb-",
      "authorship_tag": "ABX9TyP/aK8+2wZz/CDmgN8ae05J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nshah-waripari/deep_learning/blob/main/pneumothrax_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTwlNDyDTG10"
      },
      "source": [
        "**Pneumothrax Classification with pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12Mi9UdAkLa",
        "outputId": "0811d337-ff9d-4845-f14b-5d7ed8919115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# pre-requisites:\n",
        "# !pip install pretrainedmodels\n",
        "# !pip install segmentation_models_pytorch\n",
        "# NVIDIA AMP\n",
        "# %%writefile setup.sh\n",
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
        "!sh setup.sh\n",
        "\n",
        "# mount the google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")\n",
        "#!cp drive/My\\ Drive/Colab\\ Notebooks/pneumothrax_train_data/stage_2_train.csv sample_data/\n",
        "#!cp drive/My\\ Drive/Colab\\ Notebooks/train/* sample_data/train/\n",
        "#!cp drive/My\\ Drive/Colab\\ Notebooks/masks/* sample_data/mask/\n",
        "#!unzip drive/My\\ Drive/Colab\\ Notebooks/masks.zip -d drive/My\\ Drive/Colab\\ Notebooks/masks/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup.sh: 1: setup.sh: !git: not found\n",
            "setup.sh: 2: setup.sh: !pip: not found\n",
            "setup.sh: 3: setup.sh: !sh: not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV1mHC3Xw69z"
      },
      "source": [
        "#dataset loader\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "# sometimes, you will have images without an ending bit \n",
        "# this takes care of those kind of (corrupt) images Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ClassificationDataSet:\n",
        "    \"\"\"\n",
        "    A general classification dataset class that can be used for image classification tasks.\n",
        "    For example: binary classification, multi-label classification etc.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths,\n",
        "        targets,\n",
        "        resize=None,\n",
        "        augmentations=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param image_paths: list of path to images\n",
        "        :param targets: numpy array\n",
        "        :param resize: tuple, e.g. (256, 256), resizes image if not None\n",
        "        :param augmentations: albumentation augmentations\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.resize = resize\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        For a given \"item\" index, return everything we need to\n",
        "        train a given model\n",
        "        \"\"\"\n",
        "        # Use PIL to open the image\n",
        "        image = Image.open(self.image_paths[item])\n",
        "        # convert image to RGB, we have single channel images\n",
        "        image = image.convert(\"RGB\")\n",
        "        # grab correct targets\n",
        "        targets = self.targets[item]\n",
        "\n",
        "        # resize if needed\n",
        "        if self.resize is not None:\n",
        "            image = image.resize(\n",
        "                (self.resize[1], self.resize[0]),\n",
        "                resample = Image.BILINEAR\n",
        "            )\n",
        "        # convert image to numpy array\n",
        "        image = np.array(image)\n",
        "\n",
        "        # if we have albumentation augmentations\n",
        "        # add item to the image\n",
        "        if self.augmentations is not None:\n",
        "            augmented = self.augmentations(image=image)\n",
        "            image = augmented[\"image\"]\n",
        "        # pytorch expected CHW instead HWC\n",
        "        image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
        "\n",
        "        # return tensors of image and targets       \n",
        "        return {\n",
        "            \"images\": torch.tensor(image, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKBu61qexKIn"
      },
      "source": [
        "# engine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(data_loader, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    This function does the training for one epoch\n",
        "    :param data_loader: pytorch dataloader\n",
        "    :param model: pytorch model\n",
        "    :device: cuda/cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # put the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over every batch of data in the dataloader\n",
        "    for data in data_loader:\n",
        "        # extract \"image\" and \"targets\" from dataset class\n",
        "        inputs = data[\"images\"]\n",
        "        targets = data[\"targets\"]\n",
        "\n",
        "        # move the inputs and targets to cpu/cuda device\n",
        "        inputs = inputs.to(device, dtype=torch.float)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "        # zero grad the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # do the forward step\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
        "        # do the back propagation\n",
        "        loss.backward()\n",
        "        # step optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(data_loader, model, device):\n",
        "    \"\"\"\n",
        "    This function does the evaluation per epoch\n",
        "    :param data_loader: pytorch dataloader\n",
        "    :param model: pytorch model\n",
        "    :param device: cpu/cuda device    \n",
        "    \"\"\"\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # init lists to store targets and outputs\n",
        "    final_targets = []\n",
        "    final_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            inputs = data[\"images\"]\n",
        "            targets = data[\"targets\"]\n",
        "\n",
        "            # move the inputs and targets to cpu/cuda device\n",
        "            inputs = inputs.to(device, dtype=torch.float)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            # do the forward step\n",
        "            output = model(inputs)\n",
        "\n",
        "            # convert the targets and outputs to lists\n",
        "            targets = targets.detach().cpu().numpy().tolist()\n",
        "            output = output.detach().cpu().numpy().tolist()\n",
        "\n",
        "            # extend the original list\n",
        "            final_targets.extend(targets)\n",
        "            final_outputs.extend(output)\n",
        "\n",
        "    # return final outputs and targets\n",
        "    return final_outputs, final_targets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di662StYx1tS"
      },
      "source": [
        "# model\n",
        "import torch.nn as nn\n",
        "import pretrainedmodels\n",
        "\n",
        "def get_model_alexnet(pretrained):\n",
        "    if pretrained:\n",
        "        model = pretrainedmodels.__dict__[\"alexnet\"](\n",
        "            pretrained = 'imagenet'\n",
        "        )\n",
        "    else:\n",
        "        model = pretrainedmodels.__dict__[\"alexnet\"](\n",
        "            pretrained = None\n",
        "        )        \n",
        "    # add sequential layer to the model\n",
        "    model.last_linear = nn.Sequential(\n",
        "        nn.BatchNorm1d(4096),\n",
        "        nn.Dropout(p=0.25),\n",
        "        nn.Linear(in_features=4096, out_features=2048),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(in_features=2048, out_features=1)\n",
        "\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def get_model_resnet(pretrained):\n",
        "    if pretrained:\n",
        "        model = pretrainedmodels.__dict__[\"resnet18\"](\n",
        "            pretrained = 'imagenet'\n",
        "        )\n",
        "    else:\n",
        "        model = pretrainedmodels.__dict__[\"resnet18\"](\n",
        "            pretrained = None\n",
        "        )        \n",
        "    # add sequential layer to the model\n",
        "    model.last_linear = nn.Sequential(\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Dropout(p=0.25),\n",
        "        nn.Linear(in_features=512, out_features=2048),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(in_features=2048, out_features=1)\n",
        "\n",
        "    )\n",
        "    print(model)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D-LUG7Kx7Hk",
        "outputId": "1aef808f-d4a0-47c7-cb3c-d331b0709c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "import os\n",
        "import argparse\n",
        "import albumentations\n",
        "import torch\n",
        "\n",
        "# location of train.csv and image files\n",
        "data_path = 'drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# device name\n",
        "device = \"cuda\"\n",
        "\n",
        "# number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# load the dataframe\n",
        "df = pd.read_csv(os.path.join(data_path,\"pneumothrax_train_data/stage_2_train.csv\"))\n",
        "# df = pd.read_csv(os.path.join(data_path,\"stage_2_train.csv\"))\n",
        "\n",
        "#fetch all images with ImageId\n",
        "images = df.ImageId.values.tolist()\n",
        "\n",
        "# creat a list of image locations\n",
        "images = [\n",
        "    os.path.join(data_path,\"train\", i + \".png\") for i in images\n",
        "]\n",
        "print(images[0])\n",
        "targets = [0. if item==\"-1\" else 1. for item in df.EncodedPixels.values]\n",
        "print(targets[:50])\n",
        "# move the model to device\n",
        "model = get_model_resnet(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "# mean and std values of RGB channels for imagenet dataset \n",
        "# we use these pre-calculated values when we use weights # from imagenet.    \n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "# albumentation is an image augmentation library\n",
        "# that allows you to do diffrerent type of augmentations\n",
        "# simple normalization in our case\n",
        "aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Normalize(\n",
        "        mean, std, max_pixel_value=255.0, always_apply=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# simple train/valid split\n",
        "train_images, valid_images, train_targets, valid_targets = train_test_split(images, targets, stratify=targets, random_state=42)\n",
        "\n",
        "# fetch classification dataset\n",
        "train_dataset = ClassificationDataSet(train_images,\n",
        "  train_targets,\n",
        "  resize=(227, 227),\n",
        "  augmentations=aug\n",
        "  )\n",
        "\n",
        "  # create batches of data from\n",
        "  # classificatin dataset class using torch dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size = 16, shuffle=True, num_workers = 4\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# same for validation \n",
        "valid_dataset = ClassificationDataSet(valid_images,\n",
        "  valid_targets,\n",
        "  resize=(227, 227),\n",
        "  augmentations=aug\n",
        "  )\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size = 16, shuffle=True, num_workers = 4\n",
        ")\n",
        "\n",
        "# simple adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# train and print auc score for all epochs\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Training Started for epoch: {epoch}\")\n",
        "    train(train_loader, model, optimizer, device)\n",
        "    predictions, valid_targets = evaluate(valid_loader, model, device)\n",
        "    roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n",
        "    print(\n",
        "        f\"Epoch={epoch}, Valid ROC AUC = {roc_auc}\"\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/train/1.2.276.0.7230010.3.1.4.8323329.3678.1517875178.953520.png\n",
            "[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): None\n",
            "  (last_linear): Sequential(\n",
            "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): Dropout(p=0.25, inplace=False)\n",
            "    (2): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Training Started for epoch: 0\n",
            "Epoch=0, Valid ROC AUC = 0.8570188367844382\n",
            "Training Started for epoch: 1\n",
            "Epoch=1, Valid ROC AUC = 0.8910695801910867\n",
            "Training Started for epoch: 2\n",
            "Epoch=2, Valid ROC AUC = 0.8785692820652251\n",
            "Training Started for epoch: 3\n",
            "Epoch=3, Valid ROC AUC = 0.9114103499759115\n",
            "Training Started for epoch: 4\n",
            "Epoch=4, Valid ROC AUC = 0.8989315169120837\n",
            "Training Started for epoch: 5\n",
            "Epoch=5, Valid ROC AUC = 0.9038942392543514\n",
            "Training Started for epoch: 6\n",
            "Epoch=6, Valid ROC AUC = 0.9075666728676847\n",
            "Training Started for epoch: 7\n",
            "Epoch=7, Valid ROC AUC = 0.9038040859938086\n",
            "Training Started for epoch: 8\n",
            "Epoch=8, Valid ROC AUC = 0.9065139308252601\n",
            "Training Started for epoch: 9\n",
            "Epoch=9, Valid ROC AUC = 0.9033332856331954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlA0164Hj2KL"
      },
      "source": [
        "torch.save(model.state_dict(), 'drive/My Drive/Colab Notebooks/pneumothrax_model_output/pneumothrax_resnet18.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BthEGDXRqLcf"
      },
      "source": [
        "#!ls drive/My\\ Drive/Colab\\ Notebooks/pneumothrax_train_data/\n",
        "#!unzip drive/My\\ Drive/Colab\\ Notebooks/pneumothrax_train_data/train.zip -d drive/My\\ Drive/Colab\\ Notebooks/pneumothrax_train_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekleiIsY16J"
      },
      "source": [
        "**Segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpsHRsGyeBE9"
      },
      "source": [
        "# SIIM Dataset\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "\n",
        "from albumentations import (\n",
        "    Compose,\n",
        "    OneOf,\n",
        "    RandomBrightnessContrast,\n",
        "    RandomGamma,\n",
        "    ShiftScaleRotate,\n",
        ")\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "TRAIN_PATH = 'drive/My Drive/Colab Notebooks/train'\n",
        "TRAIN_MASK_PATH = 'drive/My Drive/Colab Notebooks/masks'\n",
        "\n",
        "class SIIMDataset(torch.utils.data.Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      image_ids,\n",
        "      transform = True,\n",
        "      preprocessing_fn = None\n",
        "  ):\n",
        "\n",
        "    \"\"\"\n",
        "    Dataset class for segmentation problem\n",
        "    :param iamge_ids: ids of the images as a list\n",
        "    :param transform: Boolean, no transformation in validation dataset\n",
        "    :param  preprocessing_fn: Image preprocessing function\n",
        "    \"\"\"\n",
        "\n",
        "    # create a empty dictionary to store image\n",
        "    # and mask paths\n",
        "    self.data = defaultdict(dict)\n",
        "\n",
        "    # for augmenataions\n",
        "    self.transform = transform\n",
        "    \n",
        "    # for image preprocessing\n",
        "    self.preprocessing_fn = preprocessing_fn\n",
        "\n",
        "    # albumenation augmentations\n",
        "    # shift, scale and rotate\n",
        "    # with 80% probability\n",
        "    # one of gamma and brightness/contrast\n",
        "    self.aug = Compose(\n",
        "        [\n",
        "        ShiftScaleRotate(\n",
        "            shift_limit=0.0625,\n",
        "            scale_limit=0.1,\n",
        "            rotate_limit=10,\n",
        "            p=0.8\n",
        "        ),\n",
        "        OneOf(\n",
        "            [\n",
        "              RandomGamma(gamma_limit = (90, 110)),\n",
        "              RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
        "            ],\n",
        "            p=0.5,\n",
        "        ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # iterate over image_ids to store\n",
        "    # image and mask paths\n",
        "    for counter, imgid in enumerate(image_ids):\n",
        "      files = glob.glob(os.path.join(TRAIN_PATH, imgid, \"*.png\"))\n",
        "      self.data[counter] = {\n",
        "          \"img_path\":os.path.join(TRAIN_PATH, imgid + \".png\"),\n",
        "          \"mask_path\":os.path.join(TRAIN_MASK_PATH, imgid + \".png\"),\n",
        "      }\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    # for a given item\n",
        "    # return image and mask tensors\n",
        "    img_path = self.data[item][\"img_path\"]\n",
        "    mask_path = self.data[item][\"mask_path\"]\n",
        "\n",
        "    # read image convert to RGB\n",
        "    img = Image.open(img_path)\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "    # to numpy array\n",
        "    img = np.array(img)\n",
        "\n",
        "    # read mask image\n",
        "    mask = Image.open(mask_path)\n",
        "    # to numpy array\n",
        "    mask = np.array(mask)\n",
        "\n",
        "    # convert to binary float matrix\n",
        "    mask = (mask >= 1).astype(\"float32\")\n",
        "\n",
        "    # apply transforms (to training data only)\n",
        "    if self.transform is True:\n",
        "      augmented = self.aug(image=img, mask=mask)\n",
        "      img = augmented[\"image\"]\n",
        "      mask = augmented[\"mask\"]\n",
        "\n",
        "    # preproces the image using \n",
        "    # supplied preprocessing function\n",
        "    # img = self.preprocessing_fn(img)\n",
        "\n",
        "    # return image and mask tensors\n",
        "    return {\n",
        "        \"image\":transforms.ToTensor()(img),\n",
        "        \"mask\":transforms.ToTensor()(mask).float(),\n",
        "    }\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFD6rKtHk5l2",
        "outputId": "ba3df4c5-73b5-4192-9dca-d551f3ce76a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "# train\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from apex import amp\n",
        "from collections import OrderedDict\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "TRAINING_CSV =\"drive/My Drive/Colab Notebooks/pneumothrax_train_data/stage_2_train.csv\"\n",
        "TRAINING_BATCH_SIZE = 8\n",
        "TEST_BATCH_SIZE = 4\n",
        "EPOCHS = 10\n",
        "\n",
        "# define the encoder for UNET\n",
        "ENCODER = \"resnet18\"\n",
        "\n",
        "# use imagenet pretrained weights for encoder\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "DEVICE =\"cuda\"\n",
        "\n",
        "def train(dataset, data_loader, model, criterion, optimizer):\n",
        "  \"\"\"\n",
        "  training function that trains for one epoch\n",
        "  :param dataset: dataset class (SIIMDataset)\n",
        "  :param data_loader: torch dataset loader :param model: model\n",
        "  :param criterion: loss function\n",
        "  :param optimizer: adam, sgd, etc.\n",
        "  \"\"\"\n",
        "  # put the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # calculate the number of batched\n",
        "  num_batches = int(len(dataset)/data_loader.batch_size)\n",
        "\n",
        "  # init tqdm to track progress\n",
        "  tk0 = tqdm(data_loader, total=num_batches)\n",
        "  # loop over all batches\n",
        "  for d in tk0:\n",
        "    # fetch input\n",
        "    # mask and images from dataset batch\n",
        "    inputs = d[\"image\"]\n",
        "    targets = d[\"mask\"]\n",
        "\n",
        "    # move images and masks to device\n",
        "    inputs = inputs.to(DEVICE, dtype=torch.float)\n",
        "    targets = targets.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    # zero grad the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward step the model\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # backward loss is calculated on a scaled loss\n",
        "    # context since we are using mixed precision training\n",
        "    # if you are not using mixed precision training,\n",
        "    # you can use loss.backward() and delete the following\n",
        "    # two lines of code\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "      scaled_loss.backward()\n",
        "    # step the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "  # close tqdm\n",
        "  tk0.close()\n",
        "\n",
        "def evaluate(dataset, data_loader, model):\n",
        "  \"\"\"\n",
        "  evaluation function to calculate loss on validation\n",
        "  set for one epoch\n",
        "  :param dataset: dataset class (SIIMDataset)\n",
        "  :param data_loader: torch dataset loader\n",
        "  :param model: pytorch model\n",
        "  \"\"\"\n",
        "\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # init final loss to zero\n",
        "  final_loss = 0\n",
        "\n",
        "  # calculate the number of batched\n",
        "  num_batches = int(len(dataset)/data_loader.batch_size)\n",
        "\n",
        "  # list of valid targets and outputs\n",
        "  final_outputs = []\n",
        "  final_targets = []\n",
        "\n",
        "  # init tqdm to track progress\n",
        "  tk0 = tqdm(data_loader, total=num_batches)\n",
        "  # loop over all batches\n",
        "  with torch.no_grad():\n",
        "    for d in tk0:\n",
        "      # fetch input\n",
        "      # mask and images from dataset batch\n",
        "      inputs = d[\"image\"]\n",
        "      targets = d[\"mask\"]\n",
        "      inputs = inputs.to(DEVICE, dtype=torch.float)\n",
        "      targets = targets.to(DEVICE, dtype=torch.float)\n",
        "      output = model(inputs)\n",
        "      loss = criterion(output, targets)\n",
        "      # add to final loss\n",
        "      final_loss += loss\n",
        "\n",
        "      # convert the targets and outputs to lists\n",
        "      targets = targets.detach().cpu().numpy().tolist()\n",
        "      output = output.detach().cpu().numpy().tolist()\n",
        "\n",
        "      # extend the original list\n",
        "      final_targets.extend(targets)\n",
        "      final_outputs.extend(output)\n",
        "\n",
        "  tk0.close()\n",
        "\n",
        "  # return the average loss over all batches along with predictions and targets\n",
        "  return final_loss/num_batches, final_outputs, final_targets\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  # read the training csv file\n",
        "  df = pd.read_csv(TRAINING_CSV)\n",
        "\n",
        "  # split the data into training and validation set\n",
        "  df_train, df_valid = model_selection.train_test_split(df, random_state=42, test_size=0.1)\n",
        "\n",
        "  # training and validation images list\n",
        "  training_images = df_train.ImageId.values\n",
        "  validation_images = df_valid.ImageId.values\n",
        "\n",
        "  # fetch unet model from segmentation models\n",
        "  # with specified encoder architecture\n",
        "  model = smp.Unet(\n",
        "      encoder_name = ENCODER,\n",
        "      encoder_weights = ENCODER_WEIGHTS,\n",
        "      classes = 1,\n",
        "      activation = None,\n",
        "\n",
        "  )\n",
        "\n",
        "  # segmentation model provides you with a preprocessing\n",
        "  # function that can be used for normalizing images\n",
        "  # normalization is only applied on images and not masks\n",
        "  prep_fn = smp.encoders.get_preprocessing_fn(\n",
        "      ENCODER,\n",
        "      ENCODER_WEIGHTS\n",
        "  )\n",
        "  \n",
        "  model.to(DEVICE)\n",
        "\n",
        "  # init training dataset\n",
        "  # transfor is true for training\n",
        "  train_dataset = SIIMDataset(\n",
        "      training_images,\n",
        "      transform = True,\n",
        "      preprocessing_fn = prep_fn\n",
        "  )\n",
        "\n",
        "  # wrap training dataset in torch's loader\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size = TRAINING_BATCH_SIZE,\n",
        "      shuffle = True,\n",
        "      num_workers = 12\n",
        "  )\n",
        "\n",
        "  # init validation dataset\n",
        "  # transfor is false for training\n",
        "  valid_dataset = SIIMDataset(\n",
        "      validation_images,\n",
        "      transform = False,\n",
        "      preprocessing_fn = prep_fn\n",
        "  )\n",
        "\n",
        "  # wrap validation dataset in torch's loader\n",
        "  valid_loader = torch.utils.data.DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size = TEST_BATCH_SIZE,\n",
        "      shuffle = True,\n",
        "      num_workers = 4\n",
        "  )\n",
        "\n",
        "  # define criterion\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  # use adam optimizer\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "  # reduce learning rate when we reach plateau on loss\n",
        "  scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, mode=\"min\", patience=3, verbose=True\n",
        "  )\n",
        "\n",
        "  # wrap model and optimizer with NVIDIA's apex\n",
        "  # this is used for mixed precision training\n",
        "  # if you have a GPU that supports mixed precision,\n",
        "  # this is very helpful as it will allow us to fit larger images\n",
        "  # and larger batches\n",
        "  mode, optimizer = amp.initialize(\n",
        "      model, optimizer, opt_level=\"O1\", verbosity=0\n",
        "  )\n",
        "\n",
        "  # if more then one GPU\n",
        "  if torch.cuda.device_count() > 1:\n",
        "    print(\"Using {torch.cuda.device_count()} GPUS!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "  # some logging\n",
        "  print(f\"Training batch size: {TRAINING_BATCH_SIZE}\")\n",
        "  print(f\"Test batch size: {TEST_BATCH_SIZE}\")\n",
        "  print(f\"Epochs: {EPOCHS}\")\n",
        "  print(f\"Number of training images: {len(train_dataset)}\")\n",
        "  print(f\"Number of validation images: {len(valid_dataset)}\")\n",
        "  print(f\"Encoder: {ENCODER}\")\n",
        "\n",
        "  # loop over all epochs\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch}\\n\")\n",
        "    # train for one epoch\n",
        "    train(\n",
        "        train_dataset,\n",
        "        train_loader,\n",
        "        model,\n",
        "        criterion,\n",
        "        optimizer\n",
        "    )\n",
        "    print(f\"Validation Epoch: {epoch}\")\n",
        "    # calculate validation loss\n",
        "    val_log, final_outputs, final_targets = evaluate(\n",
        "      valid_dataset,\n",
        "      valid_loader,\n",
        "      model\n",
        "    )\n",
        "    # final_outputs = sum(final_outputs, [])\n",
        "    # final_outputs = sum(final_outputs, [])\n",
        "    # final_targets = sum(final_targets, [])\n",
        "    # final_targets = sum(final_targets, [])\n",
        "    # final_targets[0]\n",
        "    # print(final_targets[0])\n",
        "    # print(final_outputs[0])\n",
        "    # roc_auc = metrics.roc_auc_score(final_outputs, final_targets)\n",
        "    accuracy = (np.array(final_outputs)==np.array(final_targets)).mean()    \n",
        "    print(\n",
        "      f\"Epoch={epoch}, Validation Accuracy = {accuracy}, Final Loss = {val_log.item()}\"\n",
        "    )\n",
        "\n",
        "    # print(f\"Validation loss: {val_log.item()}\\n\")\n",
        "    scheduler.step(val_log.item())\n",
        "    print(\"\\n\") \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1457 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training batch size: 8\n",
            "Test batch size: 4\n",
            "Epochs: 10\n",
            "Number of training images: 11658\n",
            "Number of validation images: 1296\n",
            "Encoder: resnet18\n",
            "Training epoch: 0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1458it [02:18, 10.54it/s]\n",
            "  0%|          | 0/324 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 324/324 [00:23<00:00, 13.76it/s]\n",
            "  0%|          | 0/1457 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch=0, Validation Accuracy = 0.0, Final Loss = 0.018228191882371902\n",
            "\n",
            "\n",
            "Training epoch: 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1458it [02:31,  9.64it/s]\n",
            "  0%|          | 0/324 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 65%|██████▍   | 209/324 [00:17<00:31,  3.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_tensor\u001b[0;34m(cls, storage, metadata)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mstorage_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# first construct a tensor with the correct dtype/device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 7872) is killed by signal: Killed. ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5f4c03b18b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# final_outputs = sum(final_outputs, [])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-5f4c03b18b9c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, data_loader, model)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;31m# loop over all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtk0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m       \u001b[0;31m# fetch input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;31m# mask and images from dataset batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 7872) exited unexpectedly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbvngCqe_HJZ"
      },
      "source": [
        "torch.save(model.state_dict(), 'drive/My Drive/Colab Notebooks/pneumothrax_model_output/pneumothrax_unet_resnet18.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjSCbzxYb4Vf"
      },
      "source": [
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljz3eQXRb5am"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}